import os
import json
from typing import List, Dict, Any, Optional

# --- ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ ---
_llm_provider: Optional[str] = None
_api_key: Optional[str] = None
_client = None


def set_llm_provider(config: Dict[str, Any]):
    """
    main_controllerì—ì„œ í˜¸ì¶œë˜ì–´, ì‚¬ìš©í•  LLM ê³µê¸‰ìì™€ API í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    """
    global _llm_provider, _api_key, _client

    provider = config.get("provider")
    if not provider:
        raise ValueError(
            "LLM ì„¤ì •(config.yml)ì— 'provider'ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )

    _llm_provider = provider

    if _llm_provider == "openai":
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError(
                "OpenAIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install openai'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            )
        _api_key = os.getenv("OPENAI_API_KEY")
        if not _api_key:
            raise ValueError("'OPENAI_API_KEY' í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        _client = OpenAI(api_key=_api_key)
        print("LLM ê³µê¸‰ìê°€ 'openai'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    elif _llm_provider == "anthropic":
        try:
            from anthropic import Anthropic
        except ImportError:
            raise ImportError(
                "Anthropicì„ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install anthropic'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            )

        _api_key = os.getenv("ANTHROPIC_API_KEY")
        if not _api_key:
            raise ValueError("'ANTHROPIC_API_KEY' í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        _client = Anthropic(api_key=_api_key)
        print("LLM ê³µê¸‰ìê°€ 'anthropic'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    elif _llm_provider == "mock":
        _client = "mock"
        print(
            "LLM ê³µê¸‰ìê°€ 'mock'ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ API í˜¸ì¶œì€ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤."
        )
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ìì…ë‹ˆë‹¤: {_llm_provider}")


def call_llm(messages: List[Dict[str, str]], llm_config: Dict[str, Any]) -> str:
    """
    ì„¤ì •ëœ LLM ê³µê¸‰ìë¥¼ ì‚¬ìš©í•˜ì—¬ APIë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if _client is None:
        set_llm_provider(llm_config)

    print(f"'{_llm_provider}' APIì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤...")

    try:
        if _llm_provider == "openai":
            model = llm_config.get("model", "gpt-5")
            response = _client.chat.completions.create(model=model, messages=messages)
            return response.choices[0].message.content or ""
        
        elif _llm_provider == "anthropic":
            model = llm_config.get("model", "claude-3-sonnet-20240229")
            system_prompt = ""
            if messages and messages[0]["role"] == "system":
                system_prompt = messages[0]["content"]
                user_messages = messages[1:]
            else:
                user_messages = messages

            response = _client.messages.create(
                model=model,
                system=system_prompt,
                max_tokens=4096,
                messages=user_messages,
            )
            return response.content[0].text

       # --- "ë‹µì•ˆì§€ + ë¬¸ì œì§€ ê¸°ë°˜" Mock ë¡œì§ (ìµœì¢…íŒ v4) ---
        elif _llm_provider == "mock":
            
            # ğŸ’¡ 'ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸' (ì²« ë²ˆì§¸ ë©”ì‹œì§€)ë§Œ ì—¿ë´…ë‹ˆë‹¤.
            system_prompt_str = ""
            if messages and messages[0]["role"] in ("system", "user"):
                # 'content'ê°€ Noneì¼ ìˆ˜ ìˆëŠ” ì—£ì§€ ì¼€ì´ìŠ¤ ë°©ì–´
                if messages[0].get("content"):
                    system_prompt_str = messages[0].get("content", "").lower()

            # --- (ë””ë²„ê¹…ìš© print êµ¬ë¬¸ ì œê±°) ---

            # --- ğŸ’¡ 1ìˆœìœ„: Group B (ë‹¨ì¼ LLM) ---
            if "nfrì„ ì¢…í•©ì ìœ¼ë¡œ" in system_prompt_str or "ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­" in system_prompt_str:
                fake_code = """# This is a mock code response for Group B
        def mock_group_b_function():
            pass"""
                return fake_code # ìˆœìˆ˜ ë¬¸ìì—´ ë°˜í™˜

            # --- ğŸ’¡ 2ìˆœìœ„: ê°œë°œì (Group C, D, E) ---
            # (ë°©ê¸ˆ "ì‹¬ë¬¸"ìœ¼ë¡œ ì•Œì•„ë‚¸ 'ì§„ì§œ' í‚¤ì›Œë“œë¡œ ìˆ˜ì •!)
            elif "you are a precise instruction-following expert engine for code modification" in system_prompt_str:
                # 'ë‹µì•ˆì§€(models.py)'ì˜ "DeveloperAgentOutput" ëª¨ë¸ì„ ë”°ë¦„
                fake_dev_output = {
                    "status": "SUCCESS", 
                    "final_code": "# This is mock code from the developer",
                    "log": ["Mock Developer Agent ran successfully."] 
                }
                return json.dumps(fake_dev_output)

            # --- ğŸ’¡ 3ìˆœìœ„: ì•„í‚¤í…íŠ¸ (Group D, E) ---
            # ('ì§„ì§œ' í‚¤ì›Œë“œ ì ìš© ì™„ë£Œ)
            elif "you are a world-class ai software architect" in system_prompt_str:
                # 'ë‹µì•ˆì§€(models.py)'ì˜ "IntegratedExecutionPlan" ëª¨ë¸ì„ ë”°ë¦„
                fake_plan = {
                    "work_order_id": "MOCK-WO-001", 
                    "synthesis_goal": "Balance",      
                    "reasoning_log": "This is a mock reasoning log...",
                    "instructions": [                 
                        {
                            "step": 1,
                            "action": "REPLACE",
                            "description": "Mock step 1...",
                            "target_code_block": "main.py#L1-L1",
                            "details": {
                                "refactor_type": "EXTRACT_FUNCTION", 
                                "new_function_name": "mock_extracted_function",
                                "new_function_body": "def mock_extracted_function():\n    pass"
                            },
                            "source_suggestion_ids": ["MOCK-001"],
                            "rationale": "Mock rationale.",
                            "new_code": None
                        }
                    ]
                }
                return json.dumps(fake_plan)
            
            # --- ğŸ’¡ 4ìˆœìœ„: ì „ë¬¸ê°€ (Group C, D, E) ---
            # ('ì§„ì§œ' í‚¤ì›Œë“œ ì ìš© ì™„ë£Œ)
            mock_role = None
            if "you are a world-class expert in python code performance optimization" in system_prompt_str:
                mock_role = "PerformanceExpert"
            elif "you are a world-class expert in python code readability optimization" in system_prompt_str:
                mock_role = "ReadabilityExpert"
            elif "you are a world-class expert in python code security optimization" in system_prompt_str:
                mock_role = "SecurityExpert"

            if mock_role:
                # 'ë‹µì•ˆì§€(models.py)'ì˜ "ExpertReviewReport" ëª¨ë¸ì„ ë”°ë¦„
                fake_report = [
                    {
                        "suggestion_id": f"MOCK-001-{mock_role}",
                        "agent_role": mock_role, 
                        "title": f"Mock suggestion from {mock_role}",
                        "target_code_block": "main.py#L1-L1",
                        "severity": "Low",
                        "reasoning": "This is a mock response for an Expert.",
                        "proposed_change": "pass"
                    }
                ]
                return json.dumps(fake_report)
            
            # --- ğŸ’¡ 5ìˆœìœ„: ì˜ˆì™¸ ì²˜ë¦¬ (ì–´ë–¤ í‚¤ì›Œë“œë„ ê°ì§€ë˜ì§€ ì•ŠìŒ) ---
            else:
                fallback_response = {
                    "status": "mock_fallback_unknown",
                    "log": "Mock logic failed to identify prompt. No specific mock handler was triggered."
                }
                return json.dumps(fallback_response)
        # --- ğŸ‘† Mock ë¡œì§ ë ğŸ‘† ---

        return ""

    except Exception as e:
        print(f"LLM API í˜¸ì¶œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise