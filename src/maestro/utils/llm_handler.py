import os
import json
from typing import List, Dict, Any, Optional
from openai import OpenAI, AuthenticationError

# --- [Global Token Tracker] ---
TOTAL_TOKENS = {"prompt": 0, "completion": 0}

def reset_token_usage():
    global TOTAL_TOKENS
    TOTAL_TOKENS = {"prompt": 0, "completion": 0}

def get_token_usage() -> Dict[str, int]:
    return TOTAL_TOKENS
# ------------------------------

# --- ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ ---
_llm_provider: Optional[str] = None
_api_key: Optional[str] = None
_client = None
_model_name = "gpt-4o" 
_temperature = 0.7

def set_llm_provider(config: Dict[str, Any]):
    global _llm_provider, _api_key, _client, _model_name, _temperature

    provider = config.get("provider", "openai")
    _llm_provider = provider
    _model_name = config.get("model", "gpt-4o")
    _temperature = config.get("parameters", {}).get("temperature", 0.7)

    if _llm_provider == "openai":
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError("OpenAIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install openai'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        _api_key = os.getenv("OPENAI_API_KEY")
        if not _api_key:
            raise ValueError("'OPENAI_API_KEY' í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        _client = OpenAI(api_key=_api_key)
        print(f"LLM ê³µê¸‰ìê°€ 'openai'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. (Model: {_model_name})")

    elif _llm_provider == "anthropic":
        try:
            from anthropic import Anthropic
        except ImportError:
            raise ImportError("Anthropicì„ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install anthropic'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

        _api_key = os.getenv("ANTHROPIC_API_KEY")
        if not _api_key:
            raise ValueError("'ANTHROPIC_API_KEY' í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        _client = Anthropic(api_key=_api_key)
        print(f"LLM ê³µê¸‰ìê°€ 'anthropic'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. (Model: {_model_name})")

    elif _llm_provider == "mock":
        _client = "mock"
        print("LLM ê³µê¸‰ìê°€ 'mock'ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ API í˜¸ì¶œì€ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ìì…ë‹ˆë‹¤: {_llm_provider}")


def call_llm(messages: List[Dict[str, str]], llm_config: Dict[str, Any] = None) -> str:
    global _client, TOTAL_TOKENS

    if _client is None:
        if llm_config:
            set_llm_provider(llm_config)
        else:
            set_llm_provider({"provider": "openai"})

    print(f"'{_llm_provider}' APIì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤... (Model: {_model_name})")

    try:
        if _llm_provider == "openai":
            # ìµœì‹  ëª¨ë¸(o1, gpt-5 ë“±) ëŒ€ì‘
            is_new_model = "o1" in _model_name or "gpt-5" in _model_name
            
            params = {
                "model": _model_name,
                "messages": messages,
            }
            
            # ğŸ’¡ [ìˆ˜ì •] GPT-5/o1 ëª¨ë¸ì€ temperature ì„¤ì •ì„ ì§€ì›í•˜ì§€ ì•Šê±°ë‚˜ ê³ ì •ê°’ì´ë¯€ë¡œ ì œì™¸
            if not is_new_model:
                params["temperature"] = _temperature

            if is_new_model:
                params["max_completion_tokens"] = 4096
            else:
                params["max_tokens"] = 4096

            response = _client.chat.completions.create(**params)
            
            if response.usage:
                TOTAL_TOKENS["prompt"] += response.usage.prompt_tokens
                TOTAL_TOKENS["completion"] += response.usage.completion_tokens
                
            return response.choices[0].message.content.strip()
        
        elif _llm_provider == "anthropic":
            system_prompt = ""
            user_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    system_prompt = msg["content"]
                else:
                    user_messages.append(msg)

            response = _client.messages.create(
                model=_model_name,
                system=system_prompt,
                max_tokens=4096,
                messages=user_messages,
                temperature=_temperature
            )
            
            if hasattr(response, 'usage'):
                 TOTAL_TOKENS["prompt"] += response.usage.input_tokens
                 TOTAL_TOKENS["completion"] += response.usage.output_tokens

            return response.content[0].text

        elif _llm_provider == "mock":
            return "Mock response: This is a simulated reply from the AI."

        return ""

    except AuthenticationError:
        raise ValueError("API ì¸ì¦ ì‹¤íŒ¨. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise