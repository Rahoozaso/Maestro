import os
import datetime
import json
import ast
from typing import Dict, Any, Optional


# --- ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (SWE-benchìš©) ---
try:
    from datasets import load_dataset
    import docker
except ImportError:
    pass # HumanEvalë§Œ ëŒë¦´ ë•ŒëŠ” ì—†ì–´ë„ ë¨

# --- ìœ í‹¸ë¦¬í‹° ë° ì„¤ì • ---
from maestro.utils.file_io import read_text_file, write_text_file
from maestro.utils.llm_handler import set_llm_provider

# --- ì—ì´ì „íŠ¸ ---
from maestro.agents.expert_agents import (
    PerformanceExpert,
    ReadabilityExpert,
    SecurityExpert,
)
from maestro.agents.architect_agent import ArchitectAgent
from maestro.agents.developer_agent import DeveloperAgent

# --- ë¶„ì„ ë„êµ¬ (HumanEvalìš©) ---
from maestro.tools.performance_profiler import profile_performance
from maestro.tools.readability_analyzer import analyze_readability
from maestro.tools.security_analyzer import analyze_security


class MainController:
    """
    MAESTRO í”„ë ˆì„ì›Œí¬ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡°ìœ¨í•˜ëŠ” í†µí•© ì»¨íŠ¸ë¡¤ëŸ¬ì…ë‹ˆë‹¤.
    HumanEval(ë‹¨ì¼ íŒŒì¼)ê³¼ SWE-bench(ë¦¬í¬ì§€í† ë¦¬)ë¥¼ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        set_llm_provider(config["llm"])

        # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤í™” (ê³µí†µ ì‚¬ìš©)
        self.performance_expert = PerformanceExpert(config)
        self.readability_expert = ReadabilityExpert(config)
        self.security_expert = SecurityExpert(config)
        self.architect_agent = ArchitectAgent(config)
        self.developer_agent = DeveloperAgent(config)

        print("MainController(Integrated) ì´ˆê¸°í™” ì™„ë£Œ.")

    # ====================================================
    #  CORE 1: HumanEval Workflow (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    # ====================================================
    def run_humaneval_workflow(
        self,
        source_code_path: str,
        unit_test_path: str,
        output_dir: str,
        architect_mode: str = "CoT",
        enable_retrospection: bool = True,
    ):
        """HumanEval ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë¡œì§"""
        run_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        print(f"\n===== [HumanEval] ì›Œí¬í”Œë¡œìš° ì‹œì‘ (Run ID: {run_id}) =====")

        try:
            v_gen = read_text_file(source_code_path)
            unit_tests = read_text_file(unit_test_path)
        except FileNotFoundError:
            return

        # 1. ì „ë¬¸ê°€ ìë¬¸
        print("\n--- 1ë‹¨ê³„: ì „ë¬¸ê°€ ìë¬¸ ---")
        perf_reports = self.performance_expert.run(v_gen, unit_tests)
        read_reports = self.readability_expert.run(v_gen, unit_tests)
        sec_reports = self.security_expert.run(v_gen, unit_tests)
        all_reports = (perf_reports or []) + (read_reports or []) + (sec_reports or [])

        # ì œì•ˆ ì—†ìŒ ì²˜ë¦¬ (Pass-through)
        if not all_reports:
            print("ëª¨ë“  ì „ë¬¸ê°€ê°€ ê°œì„ ì•ˆì„ ì œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ ìœ ì§€ ë° ì¢…ë£Œ.")
            quality_result = self._run_quality_gate(v_gen, v_gen)
            final_report = {
                "run_id": run_id,
                "status": "NO_CHANGES_NEEDED",
                "initial_attempt": {"quality": quality_result, "developer_log": ["No suggestions."]}
            }
            self._save_results(output_dir, v_gen, final_report)
            return

        print(f"ì´ {len(all_reports)}ê°œì˜ ê°œì„ ì•ˆ ìˆ˜ì§‘ ì™„ë£Œ.")

        # 2. ì•„í‚¤í…íŠ¸
        print("\n--- 2ë‹¨ê³„: ì•„í‚¤í…íŠ¸ ì˜ì‚¬ê²°ì • ---")
        plan = self.architect_agent.run(v_gen, all_reports, unit_tests, architect_mode=architect_mode)
        if not plan:
            print("ì•„í‚¤í…íŠ¸ê°€ ì‹¤í–‰ ê³„íš ìƒì„± ì‹¤íŒ¨. ì¢…ë£Œ.")
            return

        # 3. ê°œë°œì
        print("\n--- 3ë‹¨ê³„: ê°œë°œì êµ¬í˜„ ---")
        dev_output = self.developer_agent.run(v_gen, plan)
        if not dev_output or dev_output.status == "FAILURE":
            print("ê°œë°œì ì—ì´ì „íŠ¸ ì‹¤íŒ¨. ì¢…ë£Œ.")
            return
        v_final = dev_output.final_code

        # 4. í’ˆì§ˆ ê²€ì¦ ë° íšŒê³ 
        quality_result = self._run_quality_gate(v_gen, v_final)
        final_report = {
            "run_id": run_id,
            "initial_attempt": {"quality": quality_result, "developer_log": dev_output.log},
        }

        if (quality_result["total_score"] >= 85 and quality_result["scores"]["security"] > 0):
            print("\n í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±! ì„±ê³µ.")
            final_report["status"] = "SUCCESS_FIRST_TRY"
            self._save_results(output_dir, v_final, final_report)
            return
        elif not enable_retrospection:
            print("\n í’ˆì§ˆ ë¯¸ë‹¬ (íšŒê³  ë¹„í™œì„±). ì¢…ë£Œ.")
            final_report["status"] = "FAILURE_NO_RETROSPECTION"
            self._save_results(output_dir, v_final, final_report)
            return

        # 4.5 íšŒê³  ë£¨í”„
        print("\n--- 4.5ë‹¨ê³„: íšŒê³  ë£¨í”„ ì§„ì… ---")
        failure_feedback = f"1ì°¨ ì‹¤íŒ¨. ì´ì : {quality_result['total_score']}."
        
        if architect_mode == "RuleBased":
            final_report["status"] = "FINAL_FAILURE_RULEBASED"
            self._save_results(output_dir, v_final, final_report)
            return

        revised_plan = self.architect_agent.run(v_gen, all_reports, unit_tests, failure_feedback=failure_feedback)
        revised_dev_output = self.developer_agent.run(v_gen, revised_plan)
        
        if not revised_dev_output or revised_dev_output.status == "FAILURE":
            final_report["status"] = "FINAL_FAILURE"
            self._save_results(output_dir, v_final, final_report)
            return

        v_final_rev2 = revised_dev_output.final_code
        final_quality_result = self._run_quality_gate(v_gen, v_final_rev2)
        final_report["retrospection_attempt"] = {
            "quality": final_quality_result,
            "developer_log": revised_dev_output.log
        }
        
        status = "SUCCESS_AFTER_RETROSPECTION" if final_quality_result["total_score"] >= 85 else "FINAL_FAILURE"
        print(f"\n ìµœì¢… ê²°ê³¼: {status}")
        final_report["status"] = status
        self._save_results(output_dir, v_final_rev2, final_report)


    # ====================================================
    #  CORE 2: SWE-bench Workflow (Group E -> D í†µí•©/ì—°ê³„)
    # ====================================================
    def run_swe_workflow(self, output_base_dir: str, limit: int = 1):
        """SWE-bench ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë¡œì§ (E-D ì—°ê³„í˜•)"""
        print(f"\n===== [SWE-bench] ì›Œí¬í”Œë¡œìš° ì‹œì‘ (Limit: {limit}) =====")
        try:
            dataset = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")
        except NameError:
            print("ì˜¤ë¥˜: 'datasets' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        count = 0
        for instance in dataset:
            if count >= limit: break
            
            instance_id = instance['instance_id']
            print(f"\n>>> Processing Issue: {instance_id}")
            
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context = f"Repository: {instance['repo']}\nIssue:\n{instance['problem_statement']}"
            
            # -------------------------------------------------------
            # 1. Group B (Simple LLM) - ë…ë¦½ ì‹¤í–‰
            # -------------------------------------------------------
            self._run_group_b(instance, context, output_base_dir)

            # -------------------------------------------------------
            # 2. Group C (Rule-Based) - ë…ë¦½ ì‹¤í–‰
            # -------------------------------------------------------
            self._run_group_c(instance, context, output_base_dir)

            # -------------------------------------------------------
            # 3. Group E & D (MAESTRO Standard & Retro) - í†µí•© ì‹¤í–‰
            # -------------------------------------------------------
            # ì´ ë¡œì§ì€ E(1ì°¨ ì‹œë„)ë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ D(íšŒê³ )ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            self._run_group_e_and_d_combined(instance, context, output_base_dir)
            
            count += 1

    def _run_group_b(self, instance, context, base_dir):
        """Group B: ë‹¨ìˆœ LLM ì‹¤í–‰"""
        task_dir = os.path.join(base_dir, instance['instance_id'], "B")
        os.makedirs(task_dir, exist_ok=True)
        print(f"   [B] ì‹¤í–‰ ì¤‘... (Simple Mode)")
        
        # BëŠ” ì „ë¬¸ê°€/ì•„í‚¤í…íŠ¸ ì—†ì´ ë°”ë¡œ ê°œë°œìì—ê²Œ ë˜ì§€ê±°ë‚˜ ë‹¨ìˆœí™”ëœ ì•„í‚¤í…íŠ¸ ì‚¬ìš©
        plan = self.architect_agent.run(context, [], "N/A", synthesis_goal="Resolve Issue", architect_mode="CoT")
        if plan:
            dev_out = self.developer_agent.run(context, plan)
            if dev_out and dev_out.status == "SUCCESS":
                write_text_file(os.path.join(task_dir, "final_code.py"), dev_out.final_code)
                print(f"   [B] ì„±ê³µ: ì €ì¥ë¨")

    def _run_group_c(self, instance, context, base_dir):
        """Group C: ê·œì¹™ ê¸°ë°˜ ì•„í‚¤í…íŠ¸"""
        task_dir = os.path.join(base_dir, instance['instance_id'], "C")
        os.makedirs(task_dir, exist_ok=True)
        print(f"   [C] ì‹¤í–‰ ì¤‘... (RuleBased Mode)")

        # ì „ë¬¸ê°€ ë¶„ì„
        perf = self.performance_expert.run(context, "N/A")
        read = self.readability_expert.run(context, "N/A")
        sec = self.security_expert.run(context, "N/A")
        all_reports = (perf or []) + (read or []) + (sec or [])

        # ê·œì¹™ ê¸°ë°˜ ì•„í‚¤í…íŠ¸
        plan = self.architect_agent.run(context, all_reports, "N/A", synthesis_goal="Resolve Issue", architect_mode="RuleBased")
        
        if plan:
            dev_out = self.developer_agent.run(context, plan)
            if dev_out and dev_out.status == "SUCCESS":
                write_text_file(os.path.join(task_dir, "final_code.py"), dev_out.final_code)
                print(f"   [C] ì„±ê³µ: ì €ì¥ë¨")
            else:
                print(f"   [C] ì‹¤íŒ¨: ê°œë°œì ì˜¤ë¥˜")

    def _run_group_e_and_d_combined(self, instance, context, base_dir):
        """
        Group E(1ì°¨) -> Quality Gate -> Group D(íšŒê³ )ë¡œ ì´ì–´ì§€ëŠ” ì •êµí•œ íŒŒì´í”„ë¼ì¸
        """
        e_dir = os.path.join(base_dir, instance['instance_id'], "E")
        d_dir = os.path.join(base_dir, instance['instance_id'], "D")
        os.makedirs(e_dir, exist_ok=True)
        os.makedirs(d_dir, exist_ok=True)

        print(f"   [E & D] í†µí•© ì‹¤í–‰ ì‹œì‘...")

        # 1. ì „ë¬¸ê°€ ìë¬¸ (ê³µí†µ)
        perf = self.performance_expert.run(context, "N/A")
        read = self.readability_expert.run(context, "N/A")
        sec = self.security_expert.run(context, "N/A")
        all_reports = (perf or []) + (read or []) + (sec or [])

        # 2. 1ì°¨ ì‹œë„ (Group E ê²°ê³¼)
        print(f"   [E] 1ì°¨ ì‹œë„ (Architect & Developer)...")
        plan_v1 = self.architect_agent.run(context, all_reports, "N/A", synthesis_goal="Resolve Issue", architect_mode="CoT")
        
        if not plan_v1:
            print(f"   [E/D] ì‹¤íŒ¨: 1ì°¨ ê³„íš ìˆ˜ë¦½ ë¶ˆê°€")
            return

        dev_out_v1 = self.developer_agent.run(context, plan_v1)
        if not dev_out_v1 or dev_out_v1.status != "SUCCESS":
            print(f"   [E] ì‹¤íŒ¨: 1ì°¨ êµ¬í˜„ ì‹¤íŒ¨")
            return

        # ì €ì¥
        write_text_file(os.path.join(e_dir, "final_code.py"), dev_out_v1.final_code)
        write_text_file(os.path.join(d_dir, "final_code.py"), dev_out_v1.final_code)
        print(f"   [E] ì™„ë£Œ: 1ì°¨ ê²°ê³¼ ì €ì¥ë¨.")

        # -------------------------------------------------------
        # 3. Quality Gate & Smart Feedback (Blind Retrospection í•´ê²°)
        # -------------------------------------------------------
        print(f"   [D] 1ì°¨ ê²°ê³¼ ì •ë°€ ê²€ì‚¬ ì¤‘...")
        
        # (1) ë¬¸ë²• ë° ì •ì  ë¶„ì„ ì‹¤í–‰
        qg_result = self._run_quality_gate("N/A", dev_out_v1.final_code)
        
        # (2) í”¼ë“œë°± ìƒì„± ë¡œì§
        failure_feedback = ""
        is_syntax_error = False

        # Case A: ë¬¸ë²• ì˜¤ë¥˜ ë°œìƒ (ê°€ì¥ ì¹˜ëª…ì )
        if qg_result.get("details", {}).get("error") == "SyntaxError":
            error_msg = qg_result["details"]["message"]
            print(f"   [D] ğŸš¨ ë¬¸ë²• ì˜¤ë¥˜ ê°ì§€! ({error_msg})")
            failure_feedback = f"CRITICAL SYNTAX ERROR in previous attempt: {error_msg}. You MUST fix this syntax error immediately."
            is_syntax_error = True
            
        # Case B: ë¬¸ë²•ì€ í†µê³¼í–ˆìœ¼ë‚˜, ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ë‚˜ì¤‘ì— Docker ê²°ê³¼ë¡œ ëŒ€ì²´ë  ë¶€ë¶„)
        elif not is_syntax_error:
            print(f"   [D] ë¬¸ë²• í†µê³¼. ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸(Simulation) ì§„í–‰...")
            # TODO: ì¶”í›„ ì‹¤ì œ Docker ì‹¤í–‰ ê²°ê³¼(stderr)ë¥¼ ì—¬ê¸°ì— ë„£ì–´ì•¼ í•¨.
            # í˜„ì¬ëŠ” ê°€ìƒì˜ ImportError ìƒí™©ì„ ë¶€ì—¬í•˜ì—¬ 'Path Hallucination'ì„ ì ê²€í•˜ê²Œ ìœ ë„í•¨.
            failure_feedback = (
                "TEST FAILURE: ImportError: cannot import name '...' from partially initialized module. "
                "It seems you might be importing a non-existent file or creating a circular dependency. "
                "Check file paths and imports."
            )

        # 4. ìê¸° íšŒê³  ë£¨í”„ ì§„ì…
        print(f"   [D] âš ï¸ íšŒê³  ì‹œì‘. í”¼ë“œë°±: {failure_feedback[:100]}...")

        plan_v2 = self.architect_agent.run(
            context, all_reports, "N/A", 
            synthesis_goal="Resolve Issue", 
            failure_feedback=failure_feedback # <--- êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ì „ë‹¬
        )

        if not plan_v2:
            print(f"   [D] íšŒê³  ì‹¤íŒ¨: ìˆ˜ì • ê³„íš ìˆ˜ë¦½ ë¶ˆê°€")
            return

        dev_out_v2 = self.developer_agent.run(context, plan_v2)

        if dev_out_v2 and dev_out_v2.status == "SUCCESS":
            write_text_file(os.path.join(d_dir, "final_code.py"), dev_out_v2.final_code)
            print(f"   [D] âœ… íšŒê³  í›„ ìˆ˜ì • ì™„ë£Œ! (Smart Feedback ì ìš©ë¨)")
        else:
            print(f"   [D] íšŒê³  ì‹¤íŒ¨: ìˆ˜ì • êµ¬í˜„ ì‹¤íŒ¨")

    # ====================================================
    #  SHARED: Helper Methods
    # ====================================================
    def _run_quality_gate(
        self, original_code: str, modified_code: str
    ) -> Dict[str, Any]:
        """
        ìˆ˜ì •ëœ ì½”ë“œì˜ í’ˆì§ˆì„ ì¸¡ì •í•©ë‹ˆë‹¤.
        [ê°œì„ ë¨] 0ë‹¨ê³„: Syntax Check (ë¬¸ë²• ê²€ì‚¬)ë¥¼ í†µê³¼ ëª» í•˜ë©´ ì¦‰ì‹œ 0ì  ì²˜ë¦¬í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.
        """
        print("\n--- í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰ ---")
        
        scores = {"security": 0, "readability": 0, "performance": 0}
        
        # -------------------------------------------------------
        # [0ë‹¨ê³„] Syntax Pre-check (ë¬¸ì§€ê¸°)
        # -------------------------------------------------------
        print("0ë‹¨ê³„: Python ë¬¸ë²• ìœ íš¨ì„± ê²€ì‚¬ (Syntax Check)...")
        try:
            ast.parse(modified_code)
            print(">> ë¬¸ë²• ê²€ì‚¬ í†µê³¼ (Valid Python Code)")
        except SyntaxError as e:
            error_msg = f"SyntaxError: {e.msg} (Line {e.lineno})"
            print(f"ğŸš¨ [ì¹˜ëª…ì  ì˜¤ë¥˜] ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {error_msg}")
            print(">> ë¶„ì„ì„ ì¤‘ë‹¨í•˜ê³  0ì ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.")
            
            return {
                "total_score": 0,
                "scores": scores,
                "details": {
                    "error": "SyntaxError",
                    "message": error_msg
                },
            }
        except Exception as e:
            print(f"ğŸš¨ [ì˜¤ë¥˜] ë¬¸ë²• ê²€ì‚¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return {"total_score": 0, "scores": scores, "details": {"error": str(e)}}

        # -------------------------------------------------------
        # [1~3ë‹¨ê³„] ê¸°ì¡´ ì •ì /ë™ì  ë¶„ì„ (ë¬¸ë²• í†µê³¼ ì‹œì—ë§Œ ì‹¤í–‰)
        # -------------------------------------------------------
        
        # ë¶„ì„ ë³´ê³ ì„œ ì´ˆê¸°í™”
        sec_report = analyze_security(modified_code)
        read_report = None
        perf_report = None

        # 1. ê°€ë…ì„± ë¶„ì„
        print("1ë‹¨ê³„: ê°€ë…ì„± ë¶„ì„ ì‹œì‘ (ìˆœí™˜ ë³µì¡ë„)...")
        try:
            read_report = analyze_readability(modified_code)
            if read_report and read_report.success:
                complexity = read_report.average_complexity
                if 1 <= complexity <= 10: scores["readability"] = 30
                elif 11 <= complexity <= 20: scores["readability"] = 15
        except Exception as e:
            print(f"ê°€ë…ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œë¨): {e}")
            scores["readability"] = 0
        
        # 2. ì„±ëŠ¥ ë¶„ì„
        print("2ë‹¨ê³„: ì„±ëŠ¥ ë¶„ì„ ì‹œì‘ (ì‹¤í–‰ ì‹œê°„ ì¸¡ì •)...")
        try:
            perf_report = profile_performance(original_code, modified_code)
            if perf_report and perf_report.success:
                improvement = perf_report.improvement_percentage
                if improvement >= 15: scores["performance"] = 30
                elif 5 <= improvement < 15: scores["performance"] = 15
                elif 0 <= improvement < 5: scores["performance"] = 5
        except Exception as e:
            print(f"ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œë¨): {e}")
            scores["performance"] = 0

        # 3. ë³´ì•ˆ ì ìˆ˜ ê³„ì‚°
        if sec_report.success:
            if sec_report.highest_severity == "HIGH": scores["security"] = 0
            elif sec_report.highest_severity == "MEDIUM": scores["security"] = 15
            elif sec_report.highest_severity == "LOW": scores["security"] = 30
            else: scores["security"] = 40

        total_score = sum(scores.values())
        print(f"í’ˆì§ˆ ê²Œì´íŠ¸ ê²°ê³¼: ì´ì  = {total_score}/100")

        return {
            "total_score": total_score,
            "scores": scores,
            "details": {
                "security": sec_report,
                "readability": read_report, 
                "performance": perf_report,
            },
        }

    def _save_results(self, output_dir, final_code, report):
        """ê²°ê³¼ íŒŒì¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        write_text_file(os.path.join(output_dir, "final_code.py"), final_code)
        with open(os.path.join(output_dir, "final_report.json"), "w", encoding="utf-8") as f:
            json.dump(report, f, indent=4, ensure_ascii=False)