import os
import datetime
import json
import ast
from typing import Dict, Any, List, Union, Optional

# --- ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
try:
    from datasets import load_dataset
    import docker
except ImportError:
    pass

# --- ìœ í‹¸ë¦¬í‹° ë° ì„¤ì • ---
from maestro.utils.file_io import read_text_file, write_text_file
from maestro.utils.llm_handler import set_llm_provider, reset_token_usage, get_token_usage

# --- ì—ì´ì „íŠ¸ ---
from maestro.agents.expert_agents import PerformanceExpert, ReadabilityExpert, SecurityExpert
from maestro.agents.architect_agent import ArchitectAgent
from maestro.agents.developer_agent import DeveloperAgent
from maestro.core.data_models import ExpertReviewReport

# --- ë¶„ì„ ë„êµ¬ ---
from maestro.tools.performance_profiler import profile_performance
from maestro.tools.readability_analyzer import analyze_readability
from maestro.tools.security_analyzer import analyze_security


class MainController:
    """
    MAESTRO í†µí•© ì»¨íŠ¸ë¡¤ëŸ¬ (ì—°êµ¬ ê³„íšì„œ 5.2.3 í‰ê°€ ì§€í‘œ ì—„ê²© ì¤€ìˆ˜)
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        set_llm_provider(config["llm"])

        self.performance_expert = PerformanceExpert(config)
        self.readability_expert = ReadabilityExpert(config)
        self.security_expert = SecurityExpert(config)
        self.architect_agent = ArchitectAgent(config)
        self.developer_agent = DeveloperAgent(config)

        print("MainController(Integrated) ì´ˆê¸°í™” ì™„ë£Œ.")

    # ğŸ’¡ [í˜¸í™˜ì„±]
    def run_workflow(self, *args, **kwargs):
        return self.run_humaneval_workflow(*args, **kwargs)

    # -------------------------------------------------------
    #  Helper: ì—„ê²©í•œ í’ˆì§ˆ ê²Œì´íŠ¸ (Research Protocol 5.2.3)
    # -------------------------------------------------------
    def _run_quality_gate(self, original_code: str, modified_code: str) -> Dict[str, Any]:
        """
        [ì—„ê²© ëª¨ë“œ] ìˆ˜ì •ëœ ì½”ë“œì˜ í’ˆì§ˆì„ ì •ë°€í•˜ê²Œ ì¸¡ì •í•©ë‹ˆë‹¤.
        """
        print("\n      >>> [Quality Gate] í’ˆì§ˆ ì¸¡ì • ì‹œì‘ (Strict Mode)...")
        scores = {"security": 0, "readability": 0, "performance": 0}
        
        # 0. Syntax Check
        try:
            tree = ast.parse(modified_code)
            print("      >> ë¬¸ë²• ê²€ì‚¬ í†µê³¼")
        except SyntaxError as e:
            error_msg = f"SyntaxError: {e.msg} line {e.lineno}"
            print(f"      ğŸš¨ [ì¹˜ëª…ì ] {error_msg}")
            return {"total_score": 0, "scores": scores, "details": {"error": error_msg}}
        except Exception as e:
            return {"total_score": 0, "scores": scores, "details": {"error": str(e)}}

        # 1. ë³´ì•ˆ (Security) - ì—„ê²©í•´ì§
        sec_report = analyze_security(modified_code)
        if sec_report.success:
            if sec_report.highest_severity == "HIGH": scores["security"] = 0
            elif sec_report.highest_severity == "MEDIUM": scores["security"] = 15 # [ë³€ê²½] 30 -> 15 (ì—„ê²©)
            elif sec_report.highest_severity == "LOW": scores["security"] = 30
            else: scores["security"] = 40

        # 2. ê°€ë…ì„± (Readability) - Docstring ê²€ì‚¬ ì¶”ê°€
        try:
            read_report = analyze_readability(modified_code)
            if read_report and read_report.success:
                # (A) ë³µì¡ë„ ì ìˆ˜ (20ì  ë§Œì )
                complexity_score = 0
                avg_cc = read_report.average_complexity
                if avg_cc <= 5: complexity_score = 20
                elif avg_cc <= 10: complexity_score = 15
                elif avg_cc <= 20: complexity_score = 5
                
                # (B) Docstring ì ìˆ˜ (10ì  ë§Œì ) - ASTë¡œ ê²€ì‚¬
                docstring_score = 0
                has_docstring = False
                # ëª¨ë“ˆ ë ˆë²¨ ë˜ëŠ” í•¨ìˆ˜ ë ˆë²¨ Docstring í™•ì¸
                if ast.get_docstring(tree):
                    has_docstring = True
                else:
                    for node in ast.walk(tree):
                        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                            if ast.get_docstring(node):
                                has_docstring = True
                                break
                
                if has_docstring:
                    docstring_score = 10
                else:
                    print("      -> ê°ì : Docstring ì—†ìŒ (-10ì )")

                scores["readability"] = complexity_score + docstring_score
        except Exception: scores["readability"] = 0

        # 3. ì„±ëŠ¥ (Performance) - ê¸°ì¤€ ìƒí–¥
        perf_report = None
        if not original_code or original_code == "N/A":
             scores["performance"] = 10 
             print("      -> ë¹„êµ ëŒ€ìƒ ì—†ìŒ. ê¸°ë³¸ ì ìˆ˜ 10ì .")
        else:
            try:
                perf_report = profile_performance(original_code, modified_code)
                if perf_report and perf_report.success:
                    imp = perf_report.improvement_percentage
                    print(f"      -> ì„±ëŠ¥ ê°œì„ ìœ¨: {imp:.2f}%")
                    if imp >= 30: scores["performance"] = 30     # [ë³€ê²½] 15% -> 30% (ì—„ê²©)
                    elif imp >= 15: scores["performance"] = 20   # [ë³€ê²½] ì„¸ë¶„í™”
                    elif imp >= 5: scores["performance"] = 10
                    elif imp >= 0: scores["performance"] = 5
                    # ë§ˆì´ë„ˆìŠ¤ëŠ” 0ì 
            except Exception: 
                scores["performance"] = 0

        total = sum(scores.values())
        print(f"      >>> ê²°ê³¼: {total}/100 (Sec:{scores['security']}, Read:{scores['readability']}, Perf:{scores['performance']})")

        return {
            "total_score": total,
            "scores": scores,
            "details": {
                "security": sec_report,
                "readability": read_report, 
                "performance": perf_report
            }
        }
    
    def _save_results(self, output_dir: str, final_code: str, report: Dict[str, Any]):
        """ê²°ê³¼ ì €ì¥ (ì¢…í•© ì ìˆ˜ ê³„ì‚° í¬í•¨)"""
        os.makedirs(output_dir, exist_ok=True)
        write_text_file(os.path.join(output_dir, "final_code.py"), final_code)
        
        # ë¹„ìš© ì¶”ì 
        token_usage = get_token_usage()
        report["cost_analysis"] = {
            "prompt_tokens": token_usage["prompt"],
            "completion_tokens": token_usage["completion"],
            "estimated_cost_usd": (token_usage["prompt"] * 5 + token_usage["completion"] * 15) / 1_000_000
        }

        # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •] Comprehensive Score (Maestro Score) ê³„ì‚°
        # ë…¼ë¦¬: ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸(functional_analysis)ê°€ ì„±ê³µ(True)ì´ì–´ì•¼ë§Œ NFR ì ìˆ˜ë¥¼ ì¸ì •. 
        # ì‹¤íŒ¨ ì‹œ ì‹¤ìš©ì„±ì´ ì—†ìœ¼ë¯€ë¡œ 0ì  ë¶€ì—¬ (Hard Constraint).
        nfr_score = report.get("quality_analysis", {}).get("total_score", 0)
        func_success = report.get("functional_analysis", {}).get("success", False)
        
        final_score = nfr_score if func_success else 0
        report["maestro_score"] = final_score # ë…¼ë¬¸ì— ì‚¬ìš©ë  ìµœì¢… ì§€í‘œ

        try:
            with open(os.path.join(output_dir, "final_report.json"), "w", encoding="utf-8") as f:
                json.dump(report, f, indent=4, default=str, ensure_ascii=False)
            print(f"      -> ê²°ê³¼ ì €ì¥ ì™„ë£Œ (ë¹„ìš©: ${report['cost_analysis']['estimated_cost_usd']:.4f}, Maestro Score: {final_score})")
        except Exception as e:
            print(f"      -> ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _verify_fix_with_docker(self, instance, code_content):
        """
        Docker ìƒŒë“œë°•ìŠ¤ ê²€ì¦ (ì™„ì „ ê²©ë¦¬ & ìë™ ë’·ì •ë¦¬)
        """
        print("   [ê²€ì¦] Docker í…ŒìŠ¤íŠ¸ ì‹œë„...")
        
        # 1. ë¬¸ë²• ê²€ì‚¬
        try:
            tree = ast.parse(code_content)
        except SyntaxError as e:
            return False, f"SyntaxError in generated code: {e.msg} at line {e.lineno}"

        # 2. ì˜ì¡´ì„± ìë™ ê°ì§€
        dependencies = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for name in node.names:
                    dependencies.add(name.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    dependencies.add(node.module.split('.')[0])
        
        stdlib = {'os', 'sys', 'json', 're', 'math', 'datetime', 'time', 'typing', 'ast', 'collections', 'itertools', 'functools', 'unittest', 'dataclasses'}
        libs_to_install = list(dependencies - stdlib)
        pkg_map = {'sklearn': 'scikit-learn', 'PIL': 'Pillow', 'cv2': 'opencv-python'}
        libs_to_install = [pkg_map.get(lib, lib) for lib in libs_to_install]

        container = None
        try:
            import docker
            client = docker.from_env()
            image_name = "python:3.9" 
            
            print(f"      -> Docker({image_name}) ê²©ë¦¬ í™˜ê²½ ìƒì„± ì¤‘... (Install: {libs_to_install})")
            
            # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ êµ¬ì„±
            install_cmd = ""
            if libs_to_install:
                install_cmd = f"pip install {' '.join(libs_to_install)} --quiet --no-cache-dir && "
            
            # echoë¡œ íŒŒì¼ ìƒì„± ì‹œ íŠ¹ìˆ˜ë¬¸ì ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ cat <<EOF ë°©ì‹ ì‚¬ìš©
            setup_and_run = (
                f"{install_cmd} "
                f"cat <<EOF > run_me.py\n{code_content}\nEOF\n"
                f"python run_me.py"
            )
            
            # 3. ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ì™„ì „ ê²©ë¦¬ëœ 1íšŒìš© í™˜ê²½)
            container = client.containers.run(
                image_name,
                command=f'/bin/bash -c "{setup_and_run}"',
                detach=True,
                # ë„¤íŠ¸ì›Œí¬ í—ˆìš© (pip install ìœ„í•¨)
                network_mode="bridge" 
            )
            
            # 4. ê²°ê³¼ ëŒ€ê¸° (ìµœëŒ€ 60ì´ˆ)
            exit_code = container.wait(timeout=60)
            logs = container.logs().decode("utf-8")
            
            if exit_code['StatusCode'] == 0:
                return True, "Execution Successful (Docker)"
            else:
                return False, f"Runtime Error in Docker:\n{logs.strip()}"

        except ImportError:
            return True, "Docker Skipped (Lib missing)"
        except Exception as e:
            # íƒ€ì„ì•„ì›ƒ ë“±ìœ¼ë¡œ ì»¨í…Œì´ë„ˆê°€ ì•ˆ ì£½ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ ì²˜ë¦¬
            print(f"      -> Docker ì‹¤í–‰ ì´ìŠˆ: {e}")
            return True, f"Docker execution failed ({e})"
        
        finally:
            # ğŸ’¡ [í•µì‹¬] ì‹¤í—˜ì´ ëë‚˜ë©´ ë¬´ì¡°ê±´ ì»¨í…Œì´ë„ˆ ì‚­ì œ (í”ì  ì œê±°)
            if container:
                try:
                    container.remove(force=True)
                except Exception:
                    pass
    # ====================================================
    #  CORE 1: HumanEval Workflow
    # ====================================================
    def run_humaneval_workflow(self, source_code_path, unit_test_path, output_dir, architect_mode="CoT", enable_retrospection=True):
        run_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        print(f"\n===== [HumanEval] ì›Œí¬í”Œë¡œìš° ì‹œì‘ (Run ID: {run_id}) =====")
        try:
            v_gen = read_text_file(source_code_path)
            unit_tests = read_text_file(unit_test_path)
        except FileNotFoundError: return

        print("\n--- 1ë‹¨ê³„: ì „ë¬¸ê°€ ìë¬¸ ---")
        perf_reports = self.performance_expert.run(v_gen, unit_tests)
        read_reports = self.readability_expert.run(v_gen, unit_tests)
        sec_reports = self.security_expert.run(v_gen, unit_tests)
        all_reports = (perf_reports or []) + (read_reports or []) + (sec_reports or [])

        if not all_reports:
            print("ëª¨ë“  ì „ë¬¸ê°€ê°€ ê°œì„ ì•ˆì„ ì œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ ìœ ì§€ ë° ì¢…ë£Œ.")
            qg = self._run_quality_gate(v_gen, v_gen)
            self._save_results(output_dir, v_gen, {"run_id": run_id, "status": "NO_CHANGES", "initial": {"quality": qg}})
            return

        print(f"ì´ {len(all_reports)}ê°œì˜ ê°œì„ ì•ˆ ìˆ˜ì§‘ ì™„ë£Œ.")

        print("\n--- 2ë‹¨ê³„: ì•„í‚¤í…íŠ¸ ì˜ì‚¬ê²°ì • ---")
        plan = self.architect_agent.run(v_gen, all_reports, unit_tests, architect_mode=architect_mode)
        if not plan: return

        print("\n--- 3ë‹¨ê³„: ê°œë°œì êµ¬í˜„ ---")
        dev_output = self.developer_agent.run(v_gen, plan)
        if not dev_output or dev_output.status == "FAILURE": return
        v_final = dev_output.final_code

        # 4. í’ˆì§ˆ ê²€ì¦
        quality_result = self._run_quality_gate(v_gen, v_final)
        final_report = {
            "run_id": run_id,
            "expert_reports": [r.model_dump() for r in all_reports],
            "architect_plan": plan.model_dump(),
            "developer_log": dev_output.log,
            "quality_analysis": quality_result 
        }

        if (quality_result["total_score"] >= 85 and quality_result["scores"]["security"] > 0):
            print("\n í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±! ì„±ê³µ.")
            final_report["status"] = "SUCCESS_FIRST_TRY"
            self._save_results(output_dir, v_final, final_report)
            return
        elif not enable_retrospection:
            print("\n í’ˆì§ˆ ë¯¸ë‹¬ (íšŒê³  ë¹„í™œì„±). ì¢…ë£Œ.")
            final_report["status"] = "FAILURE_NO_RETROSPECTION"
            self._save_results(output_dir, v_final, final_report)
            return

        print("\n--- 4.5ë‹¨ê³„: íšŒê³  ë£¨í”„ ì§„ì… ---")
        if architect_mode == "RuleBased":
            final_report["status"] = "FINAL_FAILURE_RULEBASED"
            self._save_results(output_dir, v_final, final_report)
            return

        feedback = f"Score Low: {quality_result['total_score']}."
        plan_v2 = self.architect_agent.run(v_gen, all_reports, unit_tests, failure_feedback=feedback)
        
        if plan_v2:
            dev_out_v2 = self.developer_agent.run(v_gen, plan_v2)
            if dev_out_v2:
                qg_v2 = self._run_quality_gate(v_gen, dev_out_v2.final_code)
                final_report["retrospection"] = {"quality": qg_v2, "developer_log": dev_out_v2.log}
                final_report["status"] = "SUCCESS_RETRO" if qg_v2["total_score"] >= 85 else "FINAL_FAILURE"
                self._save_results(output_dir, dev_out_v2.final_code, final_report)

    # ====================================================
    #  CORE 2: SWE-bench Workflow (A -> B/C/D/E êµ¬ì¡°)
    # ====================================================
    def run_swe_workflow(self, output_base_dir: str, limit: int = 1):
        print(f"\n===== [SWE-bench] ì›Œí¬í”Œë¡œìš° ì‹œì‘ (Limit: {limit}) =====")
        try:
            dataset = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")
        except NameError: return

        count = 0
        for instance in dataset:
            if count >= limit: break
            print(f"\n>>> Processing Issue: {instance['instance_id']}")
            safe_problem = instance['problem_statement'].replace("{", "{{").replace("}", "}}")
            
            # 1. ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ (ì´ìŠˆ ì„¤ëª…)
            base_context = f"Repository: {instance['repo']}\nIssue:\n{safe_problem}"

            # -------------------------------------------------------
            # [Step 1] Group A: Baseline Code ìƒì„± (Initial Solver)
            # -------------------------------------------------------
            # ê¸°ì¡´ Group Bê°€ í•˜ë˜ ì—­í• ì„ Group Aì—ê²Œ ë¶€ì—¬
            group_a_code = self._run_group_a_generation(instance, base_context, output_base_dir)
            
            if not group_a_code:
                print("   [Critical] Group A ìƒì„± ì‹¤íŒ¨. í•´ë‹¹ ì´ìŠˆ ìŠ¤í‚µ.")
                continue

            # -------------------------------------------------------
            # [Step 2] Refactoring Context ì¤€ë¹„
            # -------------------------------------------------------
            # ì´ì œë¶€í„° B, C, D, EëŠ” ì´ìŠˆ ì„¤ëª…ë¿ë§Œ ì•„ë‹ˆë¼ Aê°€ ì§  ì½”ë“œ(v_gen)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
            refactoring_context = f"{base_context}\n\n[CURRENT CODE (v_gen)]:\n```python\n{group_a_code}\n```"
            
            # -------------------------------------------------------
            # [Step 3] Group B: Simple LLM Refactoring (New)
            # -------------------------------------------------------
            self._run_group_b_refactoring(instance, refactoring_context, output_base_dir, baseline_code=group_a_code)

            # -------------------------------------------------------
            # [Step 4] Group C: Rule-Based Refactoring
            # -------------------------------------------------------
            self._run_group_c_refactoring(instance, refactoring_context, output_base_dir, baseline_code=group_a_code)

            # -------------------------------------------------------
            # [Step 5] Group E & D: MAESTRO Refactoring
            # -------------------------------------------------------
            self._run_group_e_and_d_combined(instance, refactoring_context, output_base_dir, baseline_code=group_a_code)
            
            count += 1

    # --- [Group A] Initial Generation ---
    def _run_group_a_generation(self, instance, context, base_dir) -> Optional[str]:
        """Group A: ì´ìŠˆë¥¼ ë³´ê³  ì²˜ìŒìœ¼ë¡œ í•´ê²°ì±…ì„ ìƒì„± (ê¸°ì¡´ B ì—­í• )"""
        task_dir = os.path.join(base_dir, instance['instance_id'], "A")
        os.makedirs(task_dir, exist_ok=True)
        print(f"   [A] Baseline ìƒì„± ì¤‘... (Initial Solver)")
        
        # AëŠ” ì „ë¬¸ê°€ ì—†ì´ ë°”ë¡œ í•´ê²°ì±… ì œì•ˆ (ë‹¨ìˆœ LLM)
        dummy = [ExpertReviewReport(suggestion_id="INIT", agent_role="Dev", title="Init", target_code_block="Repo", severity="High", reasoning="Initial Fix", proposed_change="Fix")]
        plan = self.architect_agent.run(context, dummy, "N/A", "Resolve Issue", "CoT")
        
        if plan:
            dev_out = self.developer_agent.run(context, plan)
            if dev_out and dev_out.status == "SUCCESS":
                # A ê²°ê³¼ ì €ì¥
                self._save_results(task_dir, dev_out.final_code, {"run_id": "A", "status": "GENERATED"})
                print(f"   [A] ì„±ê³µ: v_gen ìƒì„± ì™„ë£Œ")
                return dev_out.final_code
        
        print(f"   [A] ì‹¤íŒ¨: ì½”ë“œ ìƒì„± ë¶ˆê°€")
        return None

    # --- [Group B] Simple Refactoring ---
    def _run_group_b_refactoring(self, instance, context, base_dir, baseline_code):
        """Group B: Aê°€ ë§Œë“  ì½”ë“œë¥¼ ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ë¡œ ë¦¬íŒ©í† ë§"""
        task_dir = os.path.join(base_dir, instance['instance_id'], "B")
        os.makedirs(task_dir, exist_ok=True)
        print(f"   [B] ì‹¤í–‰ ì¤‘... (Simple Refactoring)")
        
        # BëŠ” "ì´ ì½”ë“œë¥¼ ë” ì¢‹ê²Œ ê³ ì³ì¤˜"ë¼ëŠ” ë‹¨ìˆœ ì§€ì‹œë¥¼ ë‚´ë¦¼ (ì „ë¬¸ê°€ X)
        dummy = [ExpertReviewReport(suggestion_id="IMPROVE", agent_role="Dev", title="Improve", target_code_block="Repo", severity="Medium", reasoning="Improve NFRs", proposed_change="Refactor")]
        plan = self.architect_agent.run(context, dummy, "N/A", "Resolve Issue", "CoT")
        
        if plan:
            dev_out = self.developer_agent.run(context, plan)
            if dev_out and dev_out.status == "SUCCESS":
                qg = self._run_quality_gate(baseline_code, dev_out.final_code)
                is_valid, msg = self._verify_fix_with_docker(instance, dev_out.final_code)
                report = {"run_id": "B", "status": "DONE", "quality_analysis": qg, "functional_analysis": {"success": is_valid, "message": msg}}
                self._save_results(task_dir, dev_out.final_code, report)
                print(f"   [B] ì„±ê³µ: ì €ì¥ë¨ (ì ìˆ˜: {qg['total_score']})")

    # --- [Group C] Rule-Based Refactoring ---
    def _run_group_c_refactoring(self, instance, context, base_dir, baseline_code):
        """Group C: ê·œì¹™ ê¸°ë°˜ ë¦¬íŒ©í† ë§"""
        task_dir = os.path.join(base_dir, instance['instance_id'], "C")
        os.makedirs(task_dir, exist_ok=True)
        print(f"   [C] ì‹¤í–‰ ì¤‘... (RuleBased)")
        
        perf = self.performance_expert.run(context, "N/A")
        read = self.readability_expert.run(context, "N/A")
        sec = self.security_expert.run(context, "N/A")
        all_reports = (perf or []) + (read or []) + (sec or [])
        
        if not all_reports:
             all_reports = [ExpertReviewReport(suggestion_id="NONE", agent_role="System", title="No Issues", target_code_block="Repo", severity="Low", reasoning="None", proposed_change="Proceed")]

        plan = self.architect_agent.run(context, all_reports, "N/A", "Resolve Issue", "RuleBased")
        
        if plan:
            dev_out = self.developer_agent.run(context, plan)
            if dev_out and dev_out.status == "SUCCESS":
                qg = self._run_quality_gate(baseline_code, dev_out.final_code)
                is_valid, msg = self._verify_fix_with_docker(instance, dev_out.final_code)
                report = {"run_id": "C", "status": "DONE", "quality_analysis": qg, "functional_analysis": {"success": is_valid, "message": msg}, "architect_plan": plan.model_dump()}
                self._save_results(task_dir, dev_out.final_code, report)
                print(f"   [C] ì„±ê³µ: ì €ì¥ë¨ (ì ìˆ˜: {qg['total_score']})")

    # --- [Group E & D] MAESTRO Refactoring (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€í•˜ë˜ ì¸ì ë³€ê²½) ---
    def _run_group_e_and_d_combined(self, instance, context, base_dir, baseline_code="N/A"):
        """Group E -> D í†µí•© (Baseline ë¹„êµ í¬í•¨)"""
        # (ì´ì „ ì½”ë“œì™€ ë¡œì§ì€ ë™ì¼í•˜ì§€ë§Œ, baseline_code ì¸ìëª…ì´ í†µì¼ë¨)
        reset_token_usage()
        e_dir = os.path.join(base_dir, instance['instance_id'], "E")
        d_dir = os.path.join(base_dir, instance['instance_id'], "D")
        os.makedirs(e_dir, exist_ok=True)
        os.makedirs(d_dir, exist_ok=True)
        print(f"   [E & D] í†µí•© ì‹¤í–‰ ì‹œì‘...")
        
        perf = self.performance_expert.run(context, "N/A")
        read = self.readability_expert.run(context, "N/A")
        sec = self.security_expert.run(context, "N/A")
        all_reports = (perf or []) + (read or []) + (sec or [])
        
        if not all_reports:
            all_reports = [ExpertReviewReport(suggestion_id="NONE", agent_role="System", title="No Issues", target_code_block="Repo", severity="Low", reasoning="None", proposed_change="Proceed")]

        # 1ì°¨ ì‹œë„ (E)
        print(f"   [E] 1ì°¨ ì‹œë„...")
        plan_v1 = self.architect_agent.run(context, all_reports, "N/A", "Resolve Issue", "CoT")
        if not plan_v1: return
        dev_out_v1 = self.developer_agent.run(context, plan_v1)
        if not dev_out_v1: return

        # E ì €ì¥ (Baseline ë¹„êµ)
        qg_v1 = self._run_quality_gate(baseline_code, dev_out_v1.final_code)
        is_valid_v1, message_v1 = self._verify_fix_with_docker(instance, dev_out_v1.final_code)
        
        full_report_v1 = {
            "run_id": "E", "status": "ATTEMPT_1", "quality_analysis": qg_v1,
            "functional_analysis": {"success": is_valid_v1, "message": message_v1},
            "expert_reports": [r.model_dump() for r in all_reports],
            "architect_plan": plan_v1.model_dump(), "developer_log": dev_out_v1.log
        }
        self._save_results(e_dir, dev_out_v1.final_code, full_report_v1)
        write_text_file(os.path.join(d_dir, "final_code.py"), dev_out_v1.final_code)

        # 3. ê²€ì¦ ë° íšŒê³  (D)
        feedback_list = []
        if not is_valid_v1: feedback_list.append(f"Functional Error: {message_v1}")
        if qg_v1["total_score"] < 85: feedback_list.append(f"NFR Score Low ({qg_v1['total_score']}).")

        if not feedback_list:
            print(f"   [D] 1ì°¨ ì„±ê³µ. íšŒê³  ìƒëµ.")
            full_report_v1["run_id"] = "D"
            full_report_v1["status"] = "SUCCESS_FIRST_TRY"
            self._save_results(d_dir, dev_out_v1.final_code, full_report_v1)
        else:
            print(f"   [D] âš ï¸ íšŒê³  ì‹œì‘...")
            plan_v2 = self.architect_agent.run(context, all_reports, "N/A", "Resolve Issue", "CoT", failure_feedback="\n".join(feedback_list))
            if plan_v2:
                dev_out_v2 = self.developer_agent.run(context, plan_v2)
                if dev_out_v2:
                    qg_v2 = self._run_quality_gate(baseline_code, dev_out_v2.final_code)
                    is_valid_v2, message_v2 = self._verify_fix_with_docker(instance, dev_out_v2.final_code)
                    
                    full_report_v2 = {
                        "run_id": "D", "status": "SUCCESS_RETRO", "quality_analysis": qg_v2,
                        "functional_analysis": {"success": is_valid_v2, "message": message_v2},
                        "architect_plan": plan_v2.model_dump(), "developer_log": dev_out_v2.log,
                        "feedback_used": "\n".join(feedback_list)
                    }
                    self._save_results(d_dir, dev_out_v2.final_code, full_report_v2)
                    print(f"   [D] âœ… íšŒê³  ì™„ë£Œ.")