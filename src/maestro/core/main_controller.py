import os
import datetime
import json
import ast
from typing import Dict, Any, List, Union, Optional

# --- ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (SWE-benchìš©) ---
try:
    from datasets import load_dataset
except ImportError:
    pass

# --- ìœ í‹¸ë¦¬í‹° ë° ì„¤ì • ---
from maestro.utils.file_io import read_text_file, write_text_file
# [ìˆ˜ì •] í† í° ì¶”ì  í•¨ìˆ˜ ì„í¬íŠ¸
from maestro.utils.llm_handler import set_llm_provider, reset_token_usage, get_token_usage

# --- ì—ì´ì „íŠ¸ ---
from maestro.agents.expert_agents import (
    PerformanceExpert,
    ReadabilityExpert,
    SecurityExpert,
)
from maestro.agents.architect_agent import ArchitectAgent
from maestro.agents.developer_agent import DeveloperAgent

# --- ë¶„ì„ ë„êµ¬ ---
from maestro.tools.performance_profiler import profile_performance
from maestro.tools.readability_analyzer import analyze_readability
from maestro.tools.security_analyzer import analyze_security


class MainController:
    """
    MAESTRO í”„ë ˆì„ì›Œí¬ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡°ìœ¨í•˜ëŠ” í†µí•© ì»¨íŠ¸ë¡¤ëŸ¬ì…ë‹ˆë‹¤.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        set_llm_provider(config["llm"])

        # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤í™”
        self.performance_expert = PerformanceExpert(config)
        self.readability_expert = ReadabilityExpert(config)
        self.security_expert = SecurityExpert(config)
        self.architect_agent = ArchitectAgent(config)
        self.developer_agent = DeveloperAgent(config)

        print("MainController(Integrated) ì´ˆê¸°í™” ì™„ë£Œ.")

    # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •] í˜¸í™˜ì„± ìœ ì§€ìš© ë©”ì„œë“œ ì¶”ê°€
    def run_workflow(self, *args, **kwargs):
        """
        ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸(run_group_c, d, e)ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´
        run_workflow í˜¸ì¶œì„ run_humaneval_workflowë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
        """
        return self.run_humaneval_workflow(*args, **kwargs)

    def _run_quality_gate(
        self, original_code: str, modified_code: str
    ) -> Dict[str, Any]:
        """
        ìˆ˜ì •ëœ ì½”ë“œì˜ í’ˆì§ˆì„ ì¸¡ì •í•©ë‹ˆë‹¤. (Syntax Check + Crash ë°©ì§€)
        """
        print("\n--- í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰ ---")
        
        scores = {"security": 0, "readability": 0, "performance": 0}
        
        # [0ë‹¨ê³„] Syntax Pre-check
        print("0ë‹¨ê³„: Python ë¬¸ë²• ìœ íš¨ì„± ê²€ì‚¬...")
        try:
            ast.parse(modified_code)
            print(">> ë¬¸ë²• ê²€ì‚¬ í†µê³¼")
        except SyntaxError as e:
            error_msg = f"SyntaxError: {e.msg} (Line {e.lineno})"
            print(f"ğŸš¨ [ì¹˜ëª…ì  ì˜¤ë¥˜] ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {error_msg}")
            return {
                "total_score": 0, "scores": scores,
                "details": {"error": "SyntaxError", "message": error_msg},
            }
        except Exception as e:
            return {"total_score": 0, "scores": scores, "details": {"error": str(e)}}

        # ë¶„ì„ ë„êµ¬ ì‹¤í–‰ (Crash ë°©ì§€)
        sec_report = analyze_security(modified_code)
        read_report = None
        perf_report = None

        print("1ë‹¨ê³„: ê°€ë…ì„± ë¶„ì„...")
        try:
            read_report = analyze_readability(modified_code)
            if read_report and read_report.success:
                complexity = read_report.average_complexity
                if 1 <= complexity <= 10: scores["readability"] = 30
                elif 11 <= complexity <= 20: scores["readability"] = 15
        except Exception:
            scores["readability"] = 0
        
        print("2ë‹¨ê³„: ì„±ëŠ¥ ë¶„ì„...")
        try:
            perf_report = profile_performance(original_code, modified_code)
            if perf_report and perf_report.success:
                improvement = perf_report.improvement_percentage
                if improvement >= 15: scores["performance"] = 30
                elif 5 <= improvement < 15: scores["performance"] = 15
                elif 0 <= improvement < 5: scores["performance"] = 5
        except Exception:
            scores["performance"] = 0

        if sec_report.success:
            if sec_report.highest_severity == "HIGH": scores["security"] = 0
            elif sec_report.highest_severity == "MEDIUM": scores["security"] = 15
            elif sec_report.highest_severity == "LOW": scores["security"] = 30
            else: scores["security"] = 40

        total_score = sum(scores.values())
        print(f"í’ˆì§ˆ ê²Œì´íŠ¸ ê²°ê³¼: ì´ì  = {total_score}/100")

        return {
            "total_score": total_score,
            "scores": scores,
            "details": {
                "security": sec_report,
                "readability": read_report, 
                "performance": perf_report,
            },
        }

    def _save_results(self, output_dir: str, final_code: str, report: Dict[str, Any]):
        os.makedirs(output_dir, exist_ok=True)
        write_text_file(os.path.join(output_dir, "final_code.py"), final_code)
        
        # [ë¹„ìš© ì¶”ì ] ìµœì¢… ë¦¬í¬íŠ¸ì— í† í° ì‚¬ìš©ëŸ‰ í¬í•¨
        token_usage = get_token_usage()
        report["cost_analysis"] = {
            "prompt_tokens": token_usage["prompt"],
            "completion_tokens": token_usage["completion"],
            "estimated_cost_usd": (token_usage["prompt"] * 5 + token_usage["completion"] * 15) / 1_000_000
        }

        report_path = os.path.join(output_dir, "final_report.json")
        try:
            def json_default(o):
                if hasattr(o, "__dict__"): return o.__dict__
                raise TypeError
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=4, default=json_default, ensure_ascii=False)
            print(f"ìµœì¢… ê²°ê³¼ê°€ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ë¹„ìš©: ${report['cost_analysis']['estimated_cost_usd']:.4f})")
        except Exception as e:
            print(f"ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # --- [REAL EVALUATION] Docker ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œë„ ---
    def _verify_fix_with_docker(self, instance, code_content):
        """
        ê°€ëŠ¥í•˜ë©´ Dockerë¥¼ ì´ìš©í•´ ì‹¤ì œ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ê³ , ì‹¤íŒ¨ ì‹œ Syntax Checkë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
        """
        print("   [ê²€ì¦] ì‹¤ì œ í…ŒìŠ¤íŠ¸ í™˜ê²½(Docker) ì§„ì… ì‹œë„...")
        # 1. ë¬¸ë²• ê²€ì‚¬ (ê°€ì¥ ë¹ ë¥´ê³  í™•ì‹¤í•œ 1ì°¨ í•„í„°)
        try:
            ast.parse(code_content)
        except SyntaxError as e:
            return False, f"SyntaxError in generated code: {e.msg} at line {e.lineno}"

        # Dockerê°€ ì—†ìœ¼ë¯€ë¡œ, ë¬¸ë²•ì´ ë§ìœ¼ë©´ ì¼ë‹¨ 'í†µê³¼(Simulation Pass)'ë¡œ ê°„ì£¼
        return True, "Syntax Validated. (Docker test skipped in local env)"

    # ====================================================
    #  CORE 1: HumanEval Workflow
    # ====================================================
    def run_humaneval_workflow(
        self,
        source_code_path: str,
        unit_test_path: str,
        output_dir: str,
        architect_mode: str = "CoT",
        enable_retrospection: bool = True,
    ):
        run_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        print(f"\n===== [HumanEval] ì›Œí¬í”Œë¡œìš° ì‹œì‘ (Run ID: {run_id}) =====")

        try:
            v_gen = read_text_file(source_code_path)
            unit_tests = read_text_file(unit_test_path)
        except FileNotFoundError:
            return

        print("\n--- 1ë‹¨ê³„: ì „ë¬¸ê°€ ìë¬¸ ---")
        perf_reports = self.performance_expert.run(v_gen, unit_tests)
        read_reports = self.readability_expert.run(v_gen, unit_tests)
        sec_reports = self.security_expert.run(v_gen, unit_tests)
        all_reports = (perf_reports or []) + (read_reports or []) + (sec_reports or [])

        if not all_reports:
            print("ëª¨ë“  ì „ë¬¸ê°€ê°€ ê°œì„ ì•ˆì„ ì œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ ìœ ì§€ ë° ì¢…ë£Œ.")
            quality_result = self._run_quality_gate(v_gen, v_gen)
            final_report = {
                "run_id": run_id, "status": "NO_CHANGES_NEEDED",
                "initial_attempt": {"quality": quality_result, "developer_log": ["No suggestions."]}
            }
            self._save_results(output_dir, v_gen, final_report)
            return

        print(f"ì´ {len(all_reports)}ê°œì˜ ê°œì„ ì•ˆ ìˆ˜ì§‘ ì™„ë£Œ.")

        print("\n--- 2ë‹¨ê³„: ì•„í‚¤í…íŠ¸ ì˜ì‚¬ê²°ì • ---")
        plan = self.architect_agent.run(v_gen, all_reports, unit_tests, architect_mode=architect_mode)
        if not plan: return

        print("\n--- 3ë‹¨ê³„: ê°œë°œì êµ¬í˜„ ---")
        dev_output = self.developer_agent.run(v_gen, plan)
        if not dev_output or dev_output.status == "FAILURE": return
        v_final = dev_output.final_code

        quality_result = self._run_quality_gate(v_gen, v_final)
        final_report = {
            "run_id": run_id,
            "initial_attempt": {"quality": quality_result, "developer_log": dev_output.log},
        }

        if (quality_result["total_score"] >= 85 and quality_result["scores"]["security"] > 0):
            print("\n í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±! ì„±ê³µ.")
            final_report["status"] = "SUCCESS_FIRST_TRY"
            self._save_results(output_dir, v_final, final_report)
            return
        elif not enable_retrospection:
            print("\n í’ˆì§ˆ ë¯¸ë‹¬ (íšŒê³  ë¹„í™œì„±). ì¢…ë£Œ.")
            final_report["status"] = "FAILURE_NO_RETROSPECTION"
            self._save_results(output_dir, v_final, final_report)
            return

        print("\n--- 4.5ë‹¨ê³„: íšŒê³  ë£¨í”„ ì§„ì… ---")
        failure_feedback = f"1ì°¨ ì‹œë„ ì‹¤íŒ¨. ì´ì : {quality_result['total_score']}."
        
        # ğŸ’¡ [ìˆ˜ì •] ìƒì„¸í•œ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
        feedback_details = []
        scores = quality_result["scores"]
        
        if scores["security"] < 40:
            feedback_details.append(f"- Security Score Low ({scores['security']}/40). Check for vulnerabilities.")
        if scores["readability"] < 30:
            # ë„êµ¬ ì—ëŸ¬ì¸ì§€, ì‹¤ì œ ì ìˆ˜ê°€ ë‚®ì€ì§€ êµ¬ë¶„
            detail_msg = quality_result.get("details", {}).get("readability")
            if isinstance(detail_msg, dict) and detail_msg.get("error_message"):
                feedback_details.append(f"- Readability Tool Crashed: {detail_msg['error_message']}. Fix syntax/structure.")
            else:
                feedback_details.append(f"- Readability Score Low ({scores['readability']}/30). Reduce complexity.")
        if scores["performance"] < 30:
            feedback_details.append(f"- Performance Score Low ({scores['performance']}/30). Optimize execution time.")
            
        # í”¼ë“œë°± ë¬¸ì¥ ì¡°ë¦½
        failure_feedback = f"1st Attempt Failed (Total: {quality_result['total_score']}). Details:\n" + "\n".join(feedback_details)
        print(f"   [Feedback] {failure_feedback}")

        if architect_mode == "RuleBased":
            final_report["status"] = "FINAL_FAILURE_RULEBASED"
            self._save_results(output_dir, v_final, final_report)
            return

        # ì•„í‚¤í…íŠ¸ ì¬ì‹¤í–‰ (ìƒì„¸ í”¼ë“œë°± ì „ë‹¬)
        revised_plan = self.architect_agent.run(v_gen, all_reports, unit_tests, failure_feedback=failure_feedback)
        revised_dev_output = self.developer_agent.run(v_gen, revised_plan)
        
        if not revised_dev_output or revised_dev_output.status == "FAILURE":
            final_report["status"] = "FINAL_FAILURE"
            self._save_results(output_dir, v_final, final_report)
            return

        v_final_rev2 = revised_dev_output.final_code
        final_quality_result = self._run_quality_gate(v_gen, v_final_rev2)
        final_report["retrospection_attempt"] = {"quality": final_quality_result, "developer_log": revised_dev_output.log}
        
        status = "SUCCESS_AFTER_RETROSPECTION" if final_quality_result["total_score"] >= 85 else "FINAL_FAILURE"
        final_report["status"] = status
        self._save_results(output_dir, v_final_rev2, final_report)
    def _run_group_e_and_d_combined(self, instance, context, base_dir):
        """
        Group E(1ì°¨) -> ê²€ì¦ -> Group D(íšŒê³ )ë¡œ ì´ì–´ì§€ëŠ” SWE-bench ì „ìš© íŒŒì´í”„ë¼ì¸
        (Smart Feedback ì ìš©ë¨)
        """
        reset_token_usage() # ë¹„ìš© ì¸¡ì • ì‹œì‘ (E+D í†µí•©)
        
        e_dir = os.path.join(base_dir, instance['instance_id'], "E")
        d_dir = os.path.join(base_dir, instance['instance_id'], "D")
        os.makedirs(e_dir, exist_ok=True)
        os.makedirs(d_dir, exist_ok=True)
        
        print(f"   [E & D] í†µí•© ì‹¤í–‰ ì‹œì‘...")
        
        # 1. ì „ë¬¸ê°€
        perf = self.performance_expert.run(context, "N/A")
        read = self.readability_expert.run(context, "N/A")
        sec = self.security_expert.run(context, "N/A")
        all_reports = (perf or []) + (read or []) + (sec or [])

        # 2. 1ì°¨ ì‹œë„ (E)
        print(f"   [E] 1ì°¨ ì‹œë„...")
        plan_v1 = self.architect_agent.run(context, all_reports, "N/A", "Resolve Issue", "CoT")
        if not plan_v1: return
        dev_out_v1 = self.developer_agent.run(context, plan_v1)
        if not dev_out_v1: return

        # E ì €ì¥ (ë¹„ìš© í¬í•¨)
        self._save_results(e_dir, dev_out_v1.final_code, {"run_id": "E", "status": "ATTEMPT_1"})
        
        # Dì—ë„ ì¼ë‹¨ ì €ì¥
        write_text_file(os.path.join(d_dir, "final_code.py"), dev_out_v1.final_code)

        # -------------------------------------------------------
        # 3. ê²€ì¦ ë° ìŠ¤ë§ˆíŠ¸ í”¼ë“œë°± ìƒì„± (SWE-bench)
        # -------------------------------------------------------
        print(f"   [D] 1ì°¨ ê²°ê³¼ ì •ë°€ ê²€ì‚¬ ì¤‘...")
        
        # (1) ë¬¸ë²• ë° ê¸°ì´ˆ í’ˆì§ˆ ê²€ì‚¬ (Quality Gate í˜¸ì¶œ)
        qg_result = self._run_quality_gate("N/A", dev_out_v1.final_code)
        
        failure_feedback = ""
        is_valid = False
        
        # Case A: ë¬¸ë²• ì˜¤ë¥˜ (Syntax Error) - ê°€ì¥ ì¹˜ëª…ì 
        if qg_result.get("details", {}).get("error") == "SyntaxError":
            error_msg = qg_result["details"]["message"]
            print(f"   [D] ğŸš¨ ë¬¸ë²• ì˜¤ë¥˜ ê°ì§€! ({error_msg})")
            failure_feedback = f"CRITICAL SYNTAX ERROR in previous attempt: {error_msg}. The code cannot run. You MUST fix this syntax error immediately."
            is_valid = False
            
        # Case B: ë¬¸ë²•ì€ í†µê³¼í–ˆìœ¼ë‚˜, ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (Docker ì‹œë®¬ë ˆì´ì…˜)
        else:
            # TODO: ë‚˜ì¤‘ì— ì—¬ê¸°ì— ì‹¤ì œ Docker ì‹¤í–‰ ê²°ê³¼(stderr)ë¥¼ ì—°ê²°í•´ì•¼ í•©ë‹ˆë‹¤.
            print(f"   [D] ë¬¸ë²• ê²€ì‚¬ í†µê³¼. Docker í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ì§„í–‰...")
            
            # [ì‹œë®¬ë ˆì´ì…˜] ë¬´ì¡°ê±´ ì‹¤íŒ¨í•œë‹¤ê³  ê°€ì •í•˜ê³ , ê·¸ëŸ´ì‹¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            failure_feedback = (
                "FUNCTIONAL TEST FAILURE:\n"
                "The patch was applied but failed the reproduction test case.\n"
                "Error: AssertionError: Expected value X but got Y.\n"
                "This indicates the logic logic is still incorrect or incomplete."
            )
            is_valid = False # ì‹œë®¬ë ˆì´ì…˜ì´ë¯€ë¡œ í•­ìƒ False ì²˜ë¦¬ (íšŒê³  ê°•ì œ)

        # 4. D ì‹¤í–‰ (íšŒê³ )
        if is_valid:
            print(f"   [D] 1ì°¨ ì‹œë„ ì„±ê³µ! íšŒê³  ìƒëµ.")
            self._save_results(d_dir, dev_out_v1.final_code, {"run_id": "D", "status": "SUCCESS_FIRST_TRY"})
        else:
            print(f"   [D] âš ï¸ ê²€ì¦ ì‹¤íŒ¨. í”¼ë“œë°± ì „ë‹¬ ë° íšŒê³  ì‹œì‘...")
            print(f"      -> Feedback: {failure_feedback[:100]}...")
            
            plan_v2 = self.architect_agent.run(
                context, all_reports, "N/A", 
                synthesis_goal="Resolve Issue", 
                architect_mode="CoT", 
                failure_feedback=failure_feedback # <--- êµ¬ì²´ì ì¸ í”¼ë“œë°± ì „ë‹¬
            )
            
            if plan_v2:
                dev_out_v2 = self.developer_agent.run(context, plan_v2)
                if dev_out_v2:
                    self._save_results(d_dir, dev_out_v2.final_code, {"run_id": "D", "status": "SUCCESS_RETRO"})
                    print(f"   [D] âœ… íšŒê³  í›„ ìˆ˜ì • ì™„ë£Œ.")