import os
import datetime
import json
from typing import Dict, Any

# --- ìœ í‹¸ë¦¬í‹° ë° ì„¤ì • ---
from maestro.utils.file_io import read_yaml_file, read_text_file, write_text_file
from maestro.utils.llm_handler import set_llm_provider

# --- ì—ì´ì „íŠ¸ ---
from maestro.agents.expert_agents import PerformanceExpert, ReadabilityExpert, SecurityExpert
from maestro.agents.architect_agent import ArchitectAgent
from maestro.agents.developer_agent import DeveloperAgent

# --- ë¶„ì„ ë„êµ¬ ---
from maestro.tools.performance_profiler import profile_performance
from maestro.tools.readability_analyzer import analyze_readability
from maestro.tools.security_analyzer import analyze_security


class MainController:
    """
    MAESTRO í”„ë ˆì„ì›Œí¬ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡°ìœ¨í•˜ëŠ” ë©”ì¸ ì»¨íŠ¸ë¡¤ëŸ¬ì…ë‹ˆë‹¤.
    """
    def __init__(self, config: Dict[str, Any]):
        """
        ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì´ˆê¸°í™”í•˜ê³  ëª¨ë“  ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            config (Dict[str, Any]): 'config.yml'ì—ì„œ ë¡œë“œëœ ì„¤ì • ë”•ì…”ë„ˆë¦¬.
        """
        self.config = config
        set_llm_provider(config['llm'])  # LLM í•¸ë“¤ëŸ¬ì— API í‚¤ ë“± ì„¤ì • ì „ë‹¬

        # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤í™”
        self.performance_expert = PerformanceExpert(config)
        self.readability_expert = ReadabilityExpert(config)
        self.security_expert = SecurityExpert(config)
        self.architect_agent = ArchitectAgent(config)
        self.developer_agent = DeveloperAgent(config)
        
        print("MainController ì´ˆê¸°í™” ì™„ë£Œ. ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def _run_quality_gate(self, original_code: str, modified_code: str) -> Dict[str, Any]:
        """
        ìˆ˜ì •ëœ ì½”ë“œì˜ NFR í’ˆì§ˆì„ ì¸¡ì •í•˜ê³  ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
        """
        print("\n--- í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰ ---")
        
        # ê° ë¶„ì„ ë„êµ¬ ì‹¤í–‰
        sec_report = analyze_security(modified_code)
        read_report = analyze_readability(modified_code)
        perf_report = profile_performance(original_code, modified_code)

        scores = {"security": 0, "readability": 0, "performance": 0}

        # 1. ë³´ì•ˆ ì ìˆ˜ ê³„ì‚°
        if sec_report.success:
            if sec_report.highest_severity == "HIGH":
                scores["security"] = 0
            elif sec_report.highest_severity == "MEDIUM":
                scores["security"] = 15
            elif sec_report.highest_severity == "LOW":
                scores["security"] = 30
            else: # None
                scores["security"] = 40
        
        # 2. ê°€ë…ì„± ì ìˆ˜ ê³„ì‚°
        if read_report.success:
            complexity = read_report.average_complexity
            if 1 <= complexity <= 10:
                scores["readability"] = 30
            elif 11 <= complexity <= 20:
                scores["readability"] = 15
            else:
                scores["readability"] = 0

        # 3. ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        if perf_report.success:
            improvement = perf_report.improvement_percentage
            if improvement >= 15:
                scores["performance"] = 30
            elif 5 <= improvement < 15:
                scores["performance"] = 15
            elif 0 <= improvement < 5:
                scores["performance"] = 5
            else: # ì„±ëŠ¥ ì €í•˜
                scores["performance"] = 0
        
        total_score = sum(scores.values())
        print(f"í’ˆì§ˆ ê²Œì´íŠ¸ ê²°ê³¼: ì´ì  = {total_score}/100")
        print(f"  - ë³´ì•ˆ: {scores['security']}/40, ê°€ë…ì„±: {scores['readability']}/30, ì„±ëŠ¥: {scores['performance']}/30")

        return {
            "total_score": total_score,
            "scores": scores,
            "details": {
                "security": sec_report,
                "readability": read_report,
                "performance": perf_report
            }
        }

    def _save_results(self, run_id: str, final_code: str, report: Dict[str, Any]):
        """ì‹¤í—˜ ê²°ê³¼ë¥¼ outputs í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤."""
        output_dir = os.path.join(self.config['paths']['output_dir'], run_id)
        os.makedirs(output_dir, exist_ok=True)

        # ìµœì¢… ì½”ë“œ ì €ì¥
        write_text_file(os.path.join(output_dir, "final_code.py"), final_code)

        # ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥ (JSON)
        report_path = os.path.join(output_dir, "final_report.json")
        try:
            # dataclass ê°ì²´ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ê¸° ìœ„í•œ ì²˜ë¦¬
            def json_default(o):
                if hasattr(o, '__dict__'):
                    return o.__dict__
                raise TypeError
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=4, default=json_default, ensure_ascii=False)
            print(f"ìµœì¢… ê²°ê³¼ê°€ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


    def run_workflow(self, source_code_path: str, unit_test_path: str):
        """
        ë‹¨ì¼ ì½”ë“œ íŒŒì¼ì— ëŒ€í•œ MAESTRO ì „ì²´ ë¦¬íŒ©í† ë§ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        run_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        print(f"\n===== MAESTRO ì›Œí¬í”Œë¡œìš° ì‹œì‘ (Run ID: {run_id}) =====")

        try:
            v_gen = read_text_file(source_code_path)
            unit_tests = read_text_file(unit_test_path)
        except FileNotFoundError:
            return

        # --- 1ë‹¨ê³„: ì „ë¬¸ê°€ ìë¬¸ ---
        print("\n--- 1ë‹¨ê³„: ì „ë¬¸ê°€ ìë¬¸ ì‹œì‘ ---")
        perf_reports = self.performance_expert.run(v_gen, unit_tests)
        read_reports = self.readability_expert.run(v_gen, unit_tests)
        sec_reports = self.security_expert.run(v_gen, unit_tests)
        
        all_reports = (perf_reports or []) + (read_reports or []) + (sec_reports or [])
        if not all_reports:
            print("ëª¨ë“  ì „ë¬¸ê°€ê°€ ê°œì„ ì•ˆì„ ì œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›Œí¬í”Œë¡œìš°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        print(f"ì´ {len(all_reports)}ê°œì˜ ê°œì„ ì•ˆ ìˆ˜ì§‘ ì™„ë£Œ.")

        # --- 2ë‹¨ê³„: ì•„í‚¤í…íŠ¸ ì˜ì‚¬ê²°ì • ---
        print("\n--- 2ë‹¨ê³„: ì•„í‚¤í…íŠ¸ ì˜ì‚¬ê²°ì • ì‹œì‘ ---")
        plan = self.architect_agent.run(v_gen, all_reports, unit_tests)
        if not plan:
            print("ì•„í‚¤í…íŠ¸ê°€ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›Œí¬í”Œë¡œìš°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # --- 3ë‹¨ê³„: ê°œë°œì êµ¬í˜„ ---
        print("\n--- 3ë‹¨ê³„: ê°œë°œì êµ¬í˜„ ì‹œì‘ ---")
        dev_output = self.developer_agent.run(v_gen, plan)
        if not dev_output or dev_output.status == "FAILURE":
            print("ê°œë°œì ì—ì´ì „íŠ¸ê°€ ì½”ë“œ ìˆ˜ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›Œí¬í”Œë¡œìš°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        v_final = dev_output.final_code

        # --- 4ë‹¨ê³„: ìµœì¢… í’ˆì§ˆ ê²€ì¦ ë° ìê¸° íšŒê³  ---
        quality_result = self._run_quality_gate(v_gen, v_final)
        final_report = {
            "run_id": run_id,
            "initial_attempt": {
                "quality": quality_result,
                "developer_log": dev_output.log
            }
        }
        
        if quality_result["total_score"] >= 85 and quality_result["scores"]["security"] > 0:
            print("\nğŸ‰ í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±! ë¦¬íŒ©í† ë§ ì„±ê³µ.")
            self._save_results(run_id, v_final, final_report)
            return

        # í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ ìê¸° íšŒê³  ë£¨í”„ (ë‹¨ 1íšŒ)
        print("\n--- 4.5ë‹¨ê³„: í’ˆì§ˆ ë¯¸ë‹¬, ìê¸° íšŒê³  ë£¨í”„ ì‹œì‘ ---")
        failure_feedback = f"1ì°¨ ì‹œë„ ì‹¤íŒ¨. ì´ì : {quality_result['total_score']}. ì´ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ê³„íšì„ ìˆ˜ì •í•˜ì—¬ ë‹¤ì‹œ ì œì¶œí•˜ì„¸ìš”."
        
        # ì•„í‚¤í…íŠ¸ ì¬ì‹¤í–‰
        revised_plan = self.architect_agent.run(v_gen, all_reports, unit_tests, failure_feedback=failure_feedback)
        if not revised_plan:
            print("ì•„í‚¤í…íŠ¸ê°€ ìˆ˜ì •ëœ ê³„íšì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìµœì¢… ì‹¤íŒ¨.")
            final_report["status"] = "FINAL_FAILURE"
            self._save_results(run_id, v_final, final_report)
            return

        # ê°œë°œì ì¬ì‹¤í–‰
        revised_dev_output = self.developer_agent.run(v_gen, revised_plan)
        if not revised_dev_output or revised_dev_output.status == "FAILURE":
            print("ê°œë°œì ì—ì´ì „íŠ¸ê°€ ì¬ì‹œë„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìµœì¢… ì‹¤íŒ¨.")
            final_report["status"] = "FINAL_FAILURE"
            self._save_results(run_id, v_final, final_report)
            return
            
        v_final_rev2 = revised_dev_output.final_code
        
        # ìµœì¢… í’ˆì§ˆ ê²€ì¦ (2ì°¨)
        final_quality_result = self._run_quality_gate(v_gen, v_final_rev2)
        final_report["retrospection_attempt"] = {
            "quality": final_quality_result,
            "developer_log": revised_dev_output.log
        }
        
        if final_quality_result["total_score"] >= 85 and final_quality_result["scores"]["security"] > 0:
            print("\nğŸ‰ ìê¸° íšŒê³  í›„ í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±! ìµœì¢… ì„±ê³µ.")
            final_report["status"] = "SUCCESS_AFTER_RETROSPECTION"
            self._save_results(run_id, v_final_rev2, final_report)
        else:
            print("\nâŒ ìê¸° íšŒê³  í›„ì—ë„ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬. ìµœì¢… ì‹¤íŒ¨.")
            final_report["status"] = "FINAL_FAILURE"
            self._save_results(run_id, v_final, final_report) # 1ì°¨ ì‹œë„ ê²°ê³¼ ì €ì¥


# --- ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œë¥¼ ìœ„í•œ ì˜ˆì œ ---
if __name__ == '__main__':
    # ì˜ˆì œ ì‹¤í–‰ì„ ìœ„í•œ ì„ì‹œ íŒŒì¼ ìƒì„±
    EXAMPLE_CODE_PATH = "example_v_gen.py"
    EXAMPLE_TEST_PATH = "example_tests.py"
    
    # AIê°€ ìƒì„±í–ˆì„ ë²•í•œ, ê¸°ëŠ¥ì€ ë§ì§€ë§Œ í’ˆì§ˆì´ ë‚®ì€ ì½”ë“œ ì˜ˆì‹œ
    example_code = """
def find_common_elements(list1, list2):
    common = []
    for item1 in list1:
        # ì„±ëŠ¥ ë¬¸ì œ: list2ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ìˆœíšŒ (O(n*m))
        if item1 in list2:
            common.append(item1)
    # ê°€ë…ì„± ë¬¸ì œ: ë³µì¡í•œ ë¡œì§ì´ í•¨ìˆ˜ ë‚´ì— ì§ì ‘ í¬í•¨ë¨
    # ë³´ì•ˆ ë¬¸ì œ: ì˜ˆì œì—ì„œëŠ” ê°„ë‹¨í•˜ê²Œ ìƒëµ
    return common
"""
    
    example_tests = """
def test_find_common_elements():
    assert find_common_elements([1, 2, 3], [3, 4, 5]) == [3]
    assert find_common_elements([1, 2], [3, 4]) == []
    assert find_common_elements([1, 1, 2], [1, 3]) == [1, 1] # ì¤‘ë³µ ì²˜ë¦¬ í™•ì¸
"""
    
    write_text_file(EXAMPLE_CODE_PATH, example_code)
    write_text_file(EXAMPLE_TEST_PATH, example_tests)

    try:
        # 1. ì„¤ì • íŒŒì¼ ë¡œë“œ
        config = read_yaml_file("config.yml")
        
        # 2. ì»¨íŠ¸ë¡¤ëŸ¬ ìƒì„± ë° ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        controller = MainController(config)
        controller.run_workflow(EXAMPLE_CODE_PATH, EXAMPLE_TEST_PATH)
        
    except FileNotFoundError:
        print("\n[ì˜¤ë¥˜] 'config.yml' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— 'config.yml.example'ì„ ë³µì‚¬í•˜ì—¬ 'config.yml'ì„ ë§Œë“¤ê³ ,")
        print("ë‚´ë¶€ì— ìì‹ ì˜ LLM API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"\nì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # 3. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(EXAMPLE_CODE_PATH):
            os.remove(EXAMPLE_CODE_PATH)
        if os.path.exists(EXAMPLE_TEST_PATH):
            os.remove(EXAMPLE_TEST_PATH)
